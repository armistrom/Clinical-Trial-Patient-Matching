{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31f26000f797ab3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T00:53:10.669143Z",
     "start_time": "2025-12-04T00:53:10.629444Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import boto3\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, balanced_accuracy_score, f1_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "341cf6deadb22394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T00:53:11.044719Z",
     "start_time": "2025-12-04T00:53:10.964166Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kapilguru/Library/Python/3.9/lib/python/site-packages/boto3/compat.py:84: PythonDeprecationWarning: Boto3 will no longer support Python 3.9 starting April 29, 2026. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.10 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "AWS_REGION = \"us-east-1\"\n",
    "MAX_TOKEN_COUNT = 256\n",
    "TEMPERATURE = 0.0\n",
    "TOP_P = 1.0\n",
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6e5aaa90475037e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T00:53:11.449047Z",
     "start_time": "2025-12-04T00:53:11.440433Z"
    }
   },
   "outputs": [],
   "source": [
    "def call_bedrock(model_id, prompt: str, max_retries: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Call AWS Bedrock using the Converse API.\n",
    "    Works uniformly across different model families.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = bedrock.converse(\n",
    "                modelId=model_id,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"text\": prompt}]\n",
    "                    }\n",
    "                ],\n",
    "                inferenceConfig={\n",
    "                    \"maxTokens\": MAX_TOKEN_COUNT,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"topP\": TOP_P,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Extract text from converse API response\n",
    "            output_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "            return output_text.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                # Last attempt: re-raise\n",
    "                raise\n",
    "            # Simple exponential backoff\n",
    "            sleep_s = 2 ** attempt\n",
    "            print(f\"Error calling Bedrock ({e}), retrying in {sleep_s}s...\")\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "def additional_boolean_metrics(true_labels, pred_labels):\n",
    "    y_true = [label == 'yes' for label in true_labels]\n",
    "    y_pred = [label == 'yes' for label in pred_labels]\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "    print(f\"\\n=== Additional Boolean Question Metrics ===\")\n",
    "    print(\"AUC:\", auc)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Balanced Accuracy:\", balanced_acc)\n",
    "    print(\"Macro F1:\", macro_f1)\n",
    "    print(\"Micro F1:\", micro_f1)\n",
    "\n",
    "def additional_numeric_metrics(true_labels, pred_labels):\n",
    "    y_true = []\n",
    "    for label in true_labels:\n",
    "        try:\n",
    "            y_true.append(float(label))\n",
    "        except:\n",
    "            y_true.append(0)\n",
    "\n",
    "    y_pred = []\n",
    "    for label in pred_labels:\n",
    "        try:\n",
    "            y_pred.append(float(label))\n",
    "        except:\n",
    "            y_pred.append(0)\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(\"MSE:\", mse)\n",
    "    print(\"RMSE:\", rmse)\n",
    "\n",
    "def calculate_accuracy(true_labels, pred_labels):\n",
    "\n",
    "    def normalize_answer(val):\n",
    "        \"\"\"Normalize answer by removing punctuation and whitespace\"\"\"\n",
    "        if pd.isna(val):\n",
    "            return val  # Keep NaN as is\n",
    "\n",
    "        # Convert to string, strip whitespace and trailing periods\n",
    "        val_str = str(val).strip().rstrip('. ')\n",
    "\n",
    "        # Convert to lowercase for case-insensitive comparison\n",
    "        return val_str.lower()\n",
    "\n",
    "    true_arr = pd.Series(true_labels).values\n",
    "    pred_arr = pd.Series(pred_labels).values\n",
    "\n",
    "    correct = 0\n",
    "    total = len(true_arr)\n",
    "\n",
    "    for i in range(total):\n",
    "        true_val = true_arr[i]\n",
    "        pred_val = pred_arr[i]\n",
    "\n",
    "        # Check if both are NaN\n",
    "        true_is_nan = pd.isna(true_val)\n",
    "        pred_is_nan = pd.isna(pred_val)\n",
    "\n",
    "        if true_is_nan and pred_is_nan:\n",
    "            correct += 1\n",
    "        elif not true_is_nan and not pred_is_nan:\n",
    "            # Normalize both values before comparing\n",
    "            true_normalized = normalize_answer(true_val)\n",
    "            pred_normalized = normalize_answer(pred_val)\n",
    "\n",
    "            if true_normalized == pred_normalized:\n",
    "                correct += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a82c0e9ae7acf77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T04:52:04.288998Z",
     "start_time": "2025-12-04T00:55:58.498494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Reading ./fixed_RAG_prompt_save_c_100_k_8.csv\n",
      "********************************************************************************\n",
      "Sending 2300 prompts to Bedrock model 'meta.llama3-8b-instruct-v1:0'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 2300/2300 [08:51<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to final_meta.llama3-8b-instruct-v1:0_pred_w_gt_c_100_k_8.csv\n",
      "\n",
      "=== Global Accuracy ===\n",
      "Accuracy: 0.7883\n",
      "( NaN == NaN is counted as correct)\n",
      "\n",
      "=== Numeric Questions Accuracy ===\n",
      "Number of numeric questions: 800\n",
      "Accuracy: 0.6050\n",
      "MSE: 40460.720507125\n",
      "RMSE: 201.1485036164202\n",
      "\n",
      "=== Non-Numeric Questions Accuracy ===\n",
      "Number of non-numeric questions: 1500\n",
      "Accuracy: 0.8860\n",
      "\n",
      "=== Additional Boolean Question Metrics ===\n",
      "AUC: 0.8245784491945088\n",
      "Accuracy: 0.886\n",
      "Balanced Accuracy: 0.824578449194509\n",
      "Macro F1: 0.8423942797418584\n",
      "Micro F1: 0.886\n",
      "\n",
      "=== Accuracy by Question Type ===\n",
      "yes questions: 1500 examples, Accuracy: 0.8860\n",
      "numeric questions: 800 examples, Accuracy: 0.6050\n",
      "\n",
      "Saved per-question metrics to final_meta.llama3-8b-instruct-v1:0_acc_per_q_c_100_k_8.csv\n",
      "\n",
      "Per-question metrics:\n",
      "                                             question  n_examples  accuracy\n",
      "0   Does the note describe the patient as being un...         100      0.87\n",
      "1   Does the note describe the patient as ever bei...         100      1.00\n",
      "2   Does the note describe the patient as ever bei...         100      0.85\n",
      "3   Does the note describe the patient as ever bei...         100      1.00\n",
      "4   Does the note describe the patient as ever hav...         100      0.90\n",
      "5   Does the note describe the patient as ever hav...         100      0.83\n",
      "6   Does the note describe the patient as ever hav...         100      0.96\n",
      "7   Does the note describe the patient as ever hav...         100      0.92\n",
      "8   Does the note describe the patient as having D...         100      0.93\n",
      "9   Does the note describe the patient as having a...         100      0.92\n",
      "10  Does the note describe the patient as having a...         100      0.82\n",
      "11  Does the note describe the patient as having a...         100      0.87\n",
      "12  Does the note describe the patient as having a...         100      0.66\n",
      "13  Does the note describe the patient as having h...         100      0.84\n",
      "14  Does the note describe the patient as having p...         100      0.92\n",
      "15  What is the higest aspartate aminotransferase ...         100      0.60\n",
      "16  What is the higest serum creatinine (Creat) me...         100      0.74\n",
      "17  What is the higest total bilirubin (TotBili, B...         100      0.81\n",
      "18  What is the highest CHADS2 score mentioned? An...         100      0.45\n",
      "19  What is the highest blood glucose lab mentione...         100      0.35\n",
      "20  What is the lowest hemoglobin (HGB) mentioned ...         100      0.66\n",
      "21  What is the lowest left ventricular ejection (...         100      0.80\n",
      "22  What is the lowest platelet count (PLT) mentio...         100      0.43\n"
     ]
    }
   ],
   "source": [
    "# df = df.head(230)\n",
    "# # USE THIS TO LOOP ACROSS DIFFERENT MODELS\n",
    "# \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "#  \"amazon.titan-text-express-v1\"\n",
    "model_id = [\n",
    "    \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    \"meta.llama3-8b-instruct-v1:0\",\n",
    "    \"us.amazon.nova-pro-v1:0\"\n",
    "\n",
    "]\n",
    "CSV_PATH = [\"./fixed_RAG_prompt_save_c_50_k_15.csv\",\"./fixed_RAG_prompt_save_c_100_k_8.csv\",\"./fixed_RAG_prompt_save_c_150_k_5.csv\",\"./fixed_RAG_prompt_save_c_200_k_4.csv\",\"./fixed_RAG_prompt_save_c_300_k_2.csv\",\"./fixed_RAG_prompt_save_c_400_k_2.csv\",\"./fixed_RAG_prompt_save_c_500_k_1.csv\"]\n",
    "# model_id = [\"meta.llama3-8b-instruct-v1:0\"]\n",
    "# CSV_PATH = ['fixed_RAG_prompt_save_c_50_k_15.csv']\n",
    "\n",
    "for csv_path in CSV_PATH:\n",
    "    print('*'*80)\n",
    "    print(f\"Reading {csv_path}\")\n",
    "    print('*'*80)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    parts = csv_path.split('_')\n",
    "    c = parts[-3]\n",
    "    k = parts[-1].split('.')[0]\n",
    "\n",
    "\n",
    "    for model in model_id:\n",
    "        print(f\"Sending {len(df)} prompts to Bedrock model '{model}'...\")\n",
    "        OUTPUT_WITH_PREDS = f'final_{model}_pred_w_gt_c_{c}_k_{k}.csv'\n",
    "        METRICS_PER_QUESTION = f'final_{model}_acc_per_q_c_{c}_k_{k}.csv'\n",
    "        model_answers = []\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            prompt = row[\"prompt\"]\n",
    "            answer = call_bedrock(model, prompt)\n",
    "            model_answers.append(answer)\n",
    "        df[\"model_answer\"] = model_answers\n",
    "        df.to_csv(OUTPUT_WITH_PREDS, index=False)\n",
    "        print(f\"Saved predictions to {OUTPUT_WITH_PREDS}\")\n",
    "\n",
    "        # Normalize labels\n",
    "        df[\"true_label\"] = df[\"true_answer\"].replace({np.nan: \"NA\", \"nan\": \"NA\"})\n",
    "        df[\"pred_label\"] = df[\"model_answer\"]\n",
    "        df_eval = df.copy()\n",
    "\n",
    "\n",
    "        # print(\"\\n=== DEBUG: Checking label values ===\")\n",
    "        # print(\"Unique true_label values:\", sorted(df_eval[\"true_label\"].unique()))\n",
    "        # print(\"Unique pred_label values:\", sorted(df_eval[\"pred_label\"].unique()))\n",
    "        # print(\"\\nTrue label value counts:\")\n",
    "        # print(df_eval[\"true_label\"].value_counts().head(10))\n",
    "        # print(\"\\nPred label value counts:\")\n",
    "        # print(df_eval[\"pred_label\"].value_counts().head(10))\n",
    "\n",
    "        # Calculate global accuracy\n",
    "        global_accuracy = calculate_accuracy(\n",
    "            df_eval[\"true_label\"],\n",
    "            df_eval[\"pred_label\"]\n",
    "        )\n",
    "\n",
    "        print(\"\\n=== Global Accuracy ===\")\n",
    "        print(f\"Accuracy: {global_accuracy:.4f}\")\n",
    "        print(f\"( NaN == NaN is counted as correct)\")\n",
    "\n",
    "        # Separate numeric and non-numeric questions\n",
    "        def is_numeric(val):\n",
    "            \"\"\"Check if a value is numeric\"\"\"\n",
    "            if pd.isna(val) or val == \"NA\":\n",
    "                return False\n",
    "            try:\n",
    "                float(str(val))\n",
    "                return True\n",
    "            except (ValueError, TypeError):\n",
    "                return False\n",
    "\n",
    "        df_eval[\"is_numeric\"] = df_eval[\"true_answer\"].apply(is_numeric)\n",
    "\n",
    "        # Calculate accuracy for numeric questions\n",
    "        df_numeric = df_eval[df_eval[\"question_type\"] == 'numeric']\n",
    "        if len(df_numeric) > 0:\n",
    "            numeric_accuracy = calculate_accuracy(\n",
    "                df_numeric[\"true_label\"],\n",
    "                df_numeric[\"pred_label\"]\n",
    "            )\n",
    "            print(f\"\\n=== Numeric Questions Accuracy ===\")\n",
    "            print(f\"Number of numeric questions: {len(df_numeric)}\")\n",
    "            print(f\"Accuracy: {numeric_accuracy:.4f}\")\n",
    "            additional_numeric_metrics(df_numeric[\"true_label\"], df_numeric[\"pred_label\"])\n",
    "        else:\n",
    "            print(f\"\\n=== Numeric Questions Accuracy ===\")\n",
    "            print(f\"No numeric questions found\")\n",
    "\n",
    "        # Calculate accuracy for non-numeric questions\n",
    "        df_non_numeric = df_eval[df_eval[\"question_type\"] == 'yes']\n",
    "        if len(df_non_numeric) > 0:\n",
    "            non_numeric_accuracy = calculate_accuracy(\n",
    "                df_non_numeric[\"true_label\"],\n",
    "                df_non_numeric[\"pred_label\"]\n",
    "            )\n",
    "            print(f\"\\n=== Non-Numeric Questions Accuracy ===\")\n",
    "            print(f\"Number of non-numeric questions: {len(df_non_numeric)}\")\n",
    "            print(f\"Accuracy: {non_numeric_accuracy:.4f}\")\n",
    "            additional_boolean_metrics(df_non_numeric[\"true_label\"], df_non_numeric[\"pred_label\"])\n",
    "        else:\n",
    "            print(f\"\\n=== Non-Numeric Questions Accuracy ===\")\n",
    "            print(f\"No non-numeric questions found\")\n",
    "\n",
    "        # Calculate accuracy by question_type\n",
    "        if 'question_type' in df_eval.columns:\n",
    "            print(\"\\n=== Accuracy by Question Type ===\")\n",
    "            for qtype in df_eval['question_type'].unique():\n",
    "                df_qtype = df_eval[df_eval['question_type'] == qtype]\n",
    "                if len(df_qtype) > 0:\n",
    "                    qtype_accuracy = calculate_accuracy(\n",
    "                        df_qtype[\"true_label\"],\n",
    "                        df_qtype[\"pred_label\"]\n",
    "                    )\n",
    "                    print(f\"{qtype} questions: {len(df_qtype)} examples, Accuracy: {qtype_accuracy:.4f}\")\n",
    "\n",
    "        # Per-question metrics\n",
    "        metrics_rows = []\n",
    "        for question, g in df_eval.groupby(\"question\"):\n",
    "            if len(g) == 0:\n",
    "                continue\n",
    "\n",
    "            acc = calculate_accuracy(\n",
    "                g[\"true_label\"],\n",
    "                g[\"pred_label\"]\n",
    "            )\n",
    "\n",
    "            metrics_rows.append({\n",
    "                \"question\": question,\n",
    "                \"n_examples\": len(g),\n",
    "                \"accuracy\": acc\n",
    "            })\n",
    "\n",
    "        metrics_df = pd.DataFrame(metrics_rows)\n",
    "\n",
    "        if len(metrics_df) > 0:\n",
    "            metrics_df = metrics_df.sort_values(\"question\")\n",
    "            metrics_df.to_csv(METRICS_PER_QUESTION, index=False)\n",
    "\n",
    "            print(f\"\\nSaved per-question metrics to {METRICS_PER_QUESTION}\")\n",
    "            print(\"\\nPer-question metrics:\")\n",
    "            print(metrics_df)\n",
    "        else:\n",
    "            print(\"\\n No metrics to save!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b7eef-c7bd-4024-a88c-e95e58c58170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
